{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7d2319d-e735-47ce-aea7-29f21aac67cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.7.0+cu128\n",
      "Uninstalling torch-2.7.0+cu128:\n",
      "  Successfully uninstalled torch-2.7.0+cu128\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43c70a0-6c54-4379-b779-788b0976c8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch)\n",
      "  Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m160.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m235.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m146.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m198.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m188.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m153.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "Successfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 triton-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dee884b-704e-42db-bc1e-4f246ce4f635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.52.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (0.32.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.14)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.8.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas sentence-transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ae15f1-ca1c-471b-b1d2-7c2fb0f116c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eb95a91-4f83-44af-80db-7c0a7409b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /opt/conda/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (4.52.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (0.32.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (75.8.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.32.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "Successfully installed datasets-3.6.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ef53f7c-d38c-477c-b3a3-89e1e05cb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset # Useful for larger datasets\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fe1b6bb-7449-4caa-9033-aa9b0e61e95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu126\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version PyTorch was built with: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1eecf3e-0c57-497e-8cb4-4f79827c69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/jovyan/osdg-community-data-v2023-10-01.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "172a7e47-6c81-4f9a-9fb6-07365683aa0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sdg</th>\n",
       "      <th>labels_negative</th>\n",
       "      <th>labels_positive</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.6027/9789289342698-7-en</td>\n",
       "      <td>00021941702cd84171ff33962197ca1f</td>\n",
       "      <td>From a gender perspective, Paulgaard points ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.18356/eca72908-en</td>\n",
       "      <td>00028349a7f9b2485ff344ae44ccfd6b</td>\n",
       "      <td>Labour legislation regulates maximum working h...</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1787/9789264289062-4-en</td>\n",
       "      <td>0004eb64f96e1620cd852603d9cbe4d4</td>\n",
       "      <td>The average figure also masks large difference...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1787/3726edff-en</td>\n",
       "      <td>0005d3e8b213d9e2cb967666e1aca2e9</td>\n",
       "      <td>Applied research is directed “primarily toward...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1787/5k9b7bn5qzvd-en</td>\n",
       "      <td>0006a887475ccfa5a7f5f51d4ac83d02</td>\n",
       "      <td>The extent to which they are akin to corruptio...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42350</th>\n",
       "      <td>10.6027/9789289347204-5-en</td>\n",
       "      <td>fffc6ac25f325ab949f92d3563194b3f</td>\n",
       "      <td>Since the full capacity is not likely to be ut...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42351</th>\n",
       "      <td>10.1017/S0020589318000167</td>\n",
       "      <td>fffdf2bc9ac129ed95ef7727584aa74b</td>\n",
       "      <td>This article notes the judgment in Sophocleous...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42352</th>\n",
       "      <td>10.1787/9789264186378-8-en</td>\n",
       "      <td>fffe58dae29de6b843aff09b6e2f77f0</td>\n",
       "      <td>Groundwater quality can also be affected by co...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42353</th>\n",
       "      <td>10.1787/9789264191761-en</td>\n",
       "      <td>ffff357a454e2d7544f765cf2660e684</td>\n",
       "      <td>Remedier a ces lacunes est la recommandation p...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42354</th>\n",
       "      <td>10.18356/39dd1e2e-en</td>\n",
       "      <td>ffffcd9cc3f5d943560fe90fb9354ba5</td>\n",
       "      <td>The three diseases that resulted in the greate...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42355 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              doi                           text_id  \\\n",
       "0      10.6027/9789289342698-7-en  00021941702cd84171ff33962197ca1f   \n",
       "1            10.18356/eca72908-en  00028349a7f9b2485ff344ae44ccfd6b   \n",
       "2      10.1787/9789264289062-4-en  0004eb64f96e1620cd852603d9cbe4d4   \n",
       "3             10.1787/3726edff-en  0005d3e8b213d9e2cb967666e1aca2e9   \n",
       "4         10.1787/5k9b7bn5qzvd-en  0006a887475ccfa5a7f5f51d4ac83d02   \n",
       "...                           ...                               ...   \n",
       "42350  10.6027/9789289347204-5-en  fffc6ac25f325ab949f92d3563194b3f   \n",
       "42351   10.1017/S0020589318000167  fffdf2bc9ac129ed95ef7727584aa74b   \n",
       "42352  10.1787/9789264186378-8-en  fffe58dae29de6b843aff09b6e2f77f0   \n",
       "42353    10.1787/9789264191761-en  ffff357a454e2d7544f765cf2660e684   \n",
       "42354        10.18356/39dd1e2e-en  ffffcd9cc3f5d943560fe90fb9354ba5   \n",
       "\n",
       "                                                    text  sdg  \\\n",
       "0      From a gender perspective, Paulgaard points ou...    5   \n",
       "1      Labour legislation regulates maximum working h...   11   \n",
       "2      The average figure also masks large difference...    3   \n",
       "3      Applied research is directed “primarily toward...    9   \n",
       "4      The extent to which they are akin to corruptio...    3   \n",
       "...                                                  ...  ...   \n",
       "42350  Since the full capacity is not likely to be ut...    7   \n",
       "42351  This article notes the judgment in Sophocleous...   16   \n",
       "42352  Groundwater quality can also be affected by co...    6   \n",
       "42353  Remedier a ces lacunes est la recommandation p...    2   \n",
       "42354  The three diseases that resulted in the greate...    3   \n",
       "\n",
       "       labels_negative  labels_positive  agreement  \n",
       "0                    1                8   0.777778  \n",
       "1                    2                1   0.333333  \n",
       "2                    1                8   0.777778  \n",
       "3                    3                6   0.333333  \n",
       "4                    1                2   0.333333  \n",
       "...                ...              ...        ...  \n",
       "42350                5                4   0.111111  \n",
       "42351                2                4   0.333333  \n",
       "42352                0                3   1.000000  \n",
       "42353                4                5   0.111111  \n",
       "42354                1                7   0.750000  \n",
       "\n",
       "[42355 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bba918a5-d4bd-40f9-8712-1b0786d41d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration & SDG Mapping ---\n",
    "# IMPORTANT: Replace with your actual file path and column names\n",
    "CSV_FILE_PATH = 'your_file.csv'  # <--- REPLACE THIS\n",
    "TEXT_COLUMN_NAME = 'text'    # column with your text data\n",
    "SDG_COLUMN_NAME = 'sdg'      # column with SDG numbers 1-17\n",
    "\n",
    "# SDG Titles (as gathered previously)\n",
    "SDG_TITLES = {\n",
    "    1: \"End poverty in all its forms everywhere\",\n",
    "    2: \"End hunger, achieve food security and improved nutrition and promote sustainable agriculture\",\n",
    "    3: \"Ensure healthy lives and promote well-being for all at all ages\",\n",
    "    4: \"Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all\",\n",
    "    5: \"Achieve gender equality and empower all women and girls\",\n",
    "    6: \"Ensure availability and sustainable management of water and sanitation for all\",\n",
    "    7: \"Ensure access to affordable, reliable, sustainable and modern energy for all\",\n",
    "    8: \"Promote sustained, inclusive and sustainable economic growth, full and productive employment and decent work for all\",\n",
    "    9: \"Build resilient infrastructure, promote inclusive and sustainable industrialization and foster innovation\",\n",
    "    10: \"Reduce inequality within and among countries\",\n",
    "    11: \"Make cities and human settlements inclusive, safe, resilient and sustainable\",\n",
    "    12: \"Ensure sustainable consumption and production patterns\",\n",
    "    13: \"Take urgent action to combat climate change and its impacts\",\n",
    "    14: \"Conserve and sustainably use the oceans, seas and marine resources for sustainable development\",\n",
    "    15: \"Protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss\",\n",
    "    16: \"Promote peaceful and inclusive societies for sustainable development, provide access to justice for all and build effective, accountable and inclusive institutions at all levels\",\n",
    "    17: \"Strengthen the means of implementation and revitalize the Global Partnership for Sustainable Development\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "497a90d3-2065-4e8b-a583-318966863549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Data Inspection ---\n",
      "First 5 rows of the dataframe:\n",
      "                          doi                           text_id  \\\n",
      "0  10.6027/9789289342698-7-en  00021941702cd84171ff33962197ca1f   \n",
      "1        10.18356/eca72908-en  00028349a7f9b2485ff344ae44ccfd6b   \n",
      "2  10.1787/9789264289062-4-en  0004eb64f96e1620cd852603d9cbe4d4   \n",
      "3         10.1787/3726edff-en  0005d3e8b213d9e2cb967666e1aca2e9   \n",
      "4     10.1787/5k9b7bn5qzvd-en  0006a887475ccfa5a7f5f51d4ac83d02   \n",
      "\n",
      "                                                text  sdg  labels_negative  \\\n",
      "0  From a gender perspective, Paulgaard points ou...    5                1   \n",
      "1  Labour legislation regulates maximum working h...   11                2   \n",
      "2  The average figure also masks large difference...    3                1   \n",
      "3  Applied research is directed “primarily toward...    9                3   \n",
      "4  The extent to which they are akin to corruptio...    3                1   \n",
      "\n",
      "   labels_positive  agreement  \n",
      "0                8   0.777778  \n",
      "1                1   0.333333  \n",
      "2                8   0.777778  \n",
      "3                6   0.333333  \n",
      "4                2   0.333333  \n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42355 entries, 0 to 42354\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   doi              42355 non-null  object \n",
      " 1   text_id          42355 non-null  object \n",
      " 2   text             42355 non-null  object \n",
      " 3   sdg              42355 non-null  int64  \n",
      " 4   labels_negative  42355 non-null  int64  \n",
      " 5   labels_positive  42355 non-null  int64  \n",
      " 6   agreement        42355 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(3)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Initial Data Inspection ---\")\n",
    "print(\"First 5 rows of the dataframe:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "# Validate required columns\n",
    "if TEXT_COLUMN_NAME not in df.columns or SDG_COLUMN_NAME not in df.columns:\n",
    "    print(f\"Error: Ensure your CSV has columns named '{TEXT_COLUMN_NAME}' and '{SDG_COLUMN_NAME}'.\")\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")\n",
    "    exit()\n",
    "\n",
    "# Convert SDG column to integer if it's not already, handling potential errors\n",
    "try:\n",
    "    df[SDG_COLUMN_NAME] = pd.to_numeric(df[SDG_COLUMN_NAME], errors='raise')\n",
    "except ValueError as e:\n",
    "    print(f\"Error converting SDG column '{SDG_COLUMN_NAME}' to numeric: {e}\")\n",
    "    print(\"Please ensure this column contains valid SDG numbers (1-17).\")\n",
    "    exit()\n",
    "\n",
    "# Drop rows with NaN in text or SDG columns after conversion attempt\n",
    "df.dropna(subset=[TEXT_COLUMN_NAME, SDG_COLUMN_NAME], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18fbec96-4687-4557-9c57-1dae135e392f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data After SDG Title Mapping ---\n",
      "                          doi                           text_id  \\\n",
      "0  10.6027/9789289342698-7-en  00021941702cd84171ff33962197ca1f   \n",
      "1        10.18356/eca72908-en  00028349a7f9b2485ff344ae44ccfd6b   \n",
      "2  10.1787/9789264289062-4-en  0004eb64f96e1620cd852603d9cbe4d4   \n",
      "3         10.1787/3726edff-en  0005d3e8b213d9e2cb967666e1aca2e9   \n",
      "4     10.1787/5k9b7bn5qzvd-en  0006a887475ccfa5a7f5f51d4ac83d02   \n",
      "\n",
      "                                                text  sdg  labels_negative  \\\n",
      "0  From a gender perspective, Paulgaard points ou...    5                1   \n",
      "1  Labour legislation regulates maximum working h...   11                2   \n",
      "2  The average figure also masks large difference...    3                1   \n",
      "3  Applied research is directed “primarily toward...    9                3   \n",
      "4  The extent to which they are akin to corruptio...    3                1   \n",
      "\n",
      "   labels_positive  agreement  \\\n",
      "0                8   0.777778   \n",
      "1                1   0.333333   \n",
      "2                8   0.777778   \n",
      "3                6   0.333333   \n",
      "4                2   0.333333   \n",
      "\n",
      "                                           sdg_title  \n",
      "0  Achieve gender equality and empower all women ...  \n",
      "1  Make cities and human settlements inclusive, s...  \n",
      "2  Ensure healthy lives and promote well-being fo...  \n",
      "3  Build resilient infrastructure, promote inclus...  \n",
      "4  Ensure healthy lives and promote well-being fo...  \n",
      "\n",
      "Generated 84710 training examples (positive and negative).\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Data Preprocessing ---\n",
    "# Replace SDG numbers with official titles\n",
    "df['sdg_title'] = df[SDG_COLUMN_NAME].map(SDG_TITLES)\n",
    "\n",
    "# Filter out rows where SDG mapping might have failed (e.g., SDG number not in 1-17)\n",
    "df.dropna(subset=['sdg_title'], inplace=True)\n",
    "if df.empty:\n",
    "    print(\"Error: No valid SDG data remaining after mapping titles. Check your SDG column values.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n--- Data After SDG Title Mapping ---\")\n",
    "print(df.head())\n",
    "\n",
    "# Create training examples (text1, text2, score)\n",
    "# text1 will be the input text, text2 will be the SDG title\n",
    "# score = 1.0 for correct SDG, 0.0 for incorrect SDG\n",
    "\n",
    "all_sdg_titles = list(SDG_TITLES.values())\n",
    "train_examples = []\n",
    "\n",
    "# Create positive examples\n",
    "for _, row in df.iterrows():\n",
    "    train_examples.append(InputExample(texts=[row[TEXT_COLUMN_NAME], row['sdg_title']], label=1.0))\n",
    "\n",
    "# Create negative examples\n",
    "# For each text, pair it with a random SDG title that is NOT its true SDG\n",
    "num_negative_samples_per_positive = 1 # Create one negative sample for each positive one\n",
    "for _, row in df.iterrows():\n",
    "    original_text = row[TEXT_COLUMN_NAME]\n",
    "    true_sdg_title = row['sdg_title']\n",
    "    \n",
    "    negative_sdg_titles = [title for title in all_sdg_titles if title != true_sdg_title]\n",
    "    if not negative_sdg_titles: # Should not happen if there's more than 1 SDG\n",
    "        continue\n",
    "\n",
    "    for _ in range(num_negative_samples_per_positive):\n",
    "        if not negative_sdg_titles: break # safety\n",
    "        negative_sdg_title = random.choice(negative_sdg_titles)\n",
    "        train_examples.append(InputExample(texts=[original_text, negative_sdg_title], label=0.0))\n",
    "        # Optional: remove chosen negative to ensure variety if num_negative_samples_per_positive > 1\n",
    "        # negative_sdg_titles.remove(negative_sdg_title)\n",
    "\n",
    "\n",
    "print(f\"\\nGenerated {len(train_examples)} training examples (positive and negative).\")\n",
    "\n",
    "# Convert to DataFrame for easier splitting and balancing\n",
    "examples_df = pd.DataFrame([(ex.texts[0], ex.texts[1], ex.label) for ex in train_examples],\n",
    "                           columns=['text', 'sdg_title_candidate', 'score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60cbd871-066a-449e-bc6f-aea5ab4fd44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set size: 59296\n",
      "Validation set size: 12707\n",
      "Test set size: 12707\n",
      "\n",
      "Training set score distribution before explicit balancing (if any):\n",
      "score\n",
      "0.0    0.5\n",
      "1.0    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Processed datasets saved to 'processed_data' directory.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Data Splitting & Balancing ---\n",
    "# Split: 70% train, 15% validation, 15% test\n",
    "# First, split into train_val and test\n",
    "train_val_df, test_df = train_test_split(examples_df, test_size=0.15, stratify=examples_df['score'], random_state=42)\n",
    "\n",
    "# Then, split train_val into train and validation\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=(0.15/0.85), stratify=train_val_df['score'], random_state=42) # 0.15 / (1-0.15)\n",
    "\n",
    "print(f\"\\nTraining set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Balancing the training set (ensure equal positive and negative examples)\n",
    "# The current method of generating 1 negative per positive should already be balanced.\n",
    "# If class imbalance is still an issue for specific SDGs (not just pos/neg score),\n",
    "# more sophisticated balancing on the original data (before negative sampling) might be needed.\n",
    "# For now, we'll check the balance of 0 and 1 scores.\n",
    "print(\"\\nTraining set score distribution before explicit balancing (if any):\")\n",
    "print(train_df['score'].value_counts(normalize=True))\n",
    "\n",
    "# Save the processed datasets\n",
    "output_dir = \"processed_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "train_df.to_csv(os.path.join(output_dir, \"train_set.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(output_dir, \"validation_set.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(output_dir, \"test_set.csv\"), index=False)\n",
    "print(f\"\\nProcessed datasets saved to '{output_dir}' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22dfaaf4-9cdf-4eef-b911-dc66aa260192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Fine-tuning ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3414cc3f73ef49e585a0a447f5855d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3706' max='3706' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3706/3706 32:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Sts-dev Pearson Cosine</th>\n",
       "      <th>Sts-dev Spearman Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.820134</td>\n",
       "      <td>0.791458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.845762</td>\n",
       "      <td>0.808162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.855036</td>\n",
       "      <td>0.814432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3706</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858982</td>\n",
       "      <td>0.817386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fine-tuning complete. Model saved to 'output/sbert_finetuned_sdg'.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Model Fine-tuning ---\n",
    "# Define the model\n",
    "model_name = 'allenai-specter' # A good starting SBERT model\n",
    "model = SentenceTransformer(model_name)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Prepare datasets for SBERT\n",
    "train_samples_sbert = [InputExample(texts=[row['text'], row['sdg_title_candidate']], label=row['score']) for _, row in train_df.iterrows()]\n",
    "val_samples_sbert = [InputExample(texts=[row['text'], row['sdg_title_candidate']], label=row['score']) for _, row in val_df.iterrows()]\n",
    "# test_samples_sbert will be used for evaluation later\n",
    "\n",
    "# Define DataLoader\n",
    "train_dataloader = DataLoader(train_samples_sbert, shuffle=True, batch_size=16)\n",
    "\n",
    "# Define loss function\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Define evaluator for validation set\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(val_samples_sbert, name='sts-dev')\n",
    "\n",
    "# Fine-tune the model\n",
    "num_epochs = 1 # Adjust as needed, more epochs can lead to better performance or overfitting\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) # 10% of train data for warm-up\n",
    "\n",
    "print(\"\\n--- Starting Model Fine-tuning ---\")\n",
    "model_save_path = 'output/sbert_finetuned_sdg'\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000, # Evaluate every 1000 steps\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path,\n",
    "          show_progress_bar=True)\n",
    "\n",
    "print(f\"Model fine-tuning complete. Model saved to '{model_save_path}'.\")\n",
    "\n",
    "# Load the fine-tuned model (optional, if you want to ensure it loads correctly or use it immediately)\n",
    "# model = SentenceTransformer(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9e065a0-68e2-4e5b-9d1a-b44a6b87015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Model on Test Set ---\n",
      "Evaluating on 6354 unique texts from the test split.\n",
      "\n",
      "--- Classification Metrics ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Classification Metrics ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Ensure labels for classification_report are present in both true and predicted\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m unique_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(true_labels, predictions, labels\u001b[38;5;241m=\u001b[39munique_labels, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Confusion Matrix ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "# --- 6. Evaluation ---\n",
    "model_save_path = 'output/sbert_finetuned_sdg'\n",
    "model = SentenceTransformer(model_save_path)\n",
    "print(\"\\n--- Evaluating Model on Test Set ---\")\n",
    "\n",
    "# We need to predict which SDG is the best match for each text in the original test portion.\n",
    "# This is slightly different from the pairwise training.\n",
    "# For each text, we'll compute similarity with ALL SDG titles and pick the highest.\n",
    "\n",
    "# Get unique texts from the test set (original texts, not pairs)\n",
    "# We need to go back to the original data split before creating positive/negative pairs for evaluation.\n",
    "# For simplicity, let's use the original 'df' and split it again to get a 'clean' test set of (text, true_sdg_title)\n",
    "\n",
    "# Create a 'clean' version of data with (text, true_sdg_title)\n",
    "original_data_for_eval = df[[TEXT_COLUMN_NAME, 'sdg_title']].drop_duplicates()\n",
    "\n",
    "# Split this original data for a clean test set (texts and their true SDG)\n",
    "_, original_test_data = train_test_split(original_data_for_eval, test_size=0.15, random_state=42) # Use same random_state as before\n",
    "\n",
    "if original_test_data.empty:\n",
    "    print(\"Error: The original test data split resulted in an empty set. Check data size and split proportions.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Evaluating on {len(original_test_data)} unique texts from the test split.\")\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "all_sdg_title_list_eval = list(SDG_TITLES.values()) # Ensure this is just the list of titles\n",
    "\n",
    "# Encode all SDG titles once\n",
    "sdg_title_embeddings = model.encode(all_sdg_title_list_eval, convert_to_tensor=True)\n",
    "\n",
    "ii=1\n",
    "for _, row in original_test_data.iterrows():\n",
    "    text_to_evaluate = row[TEXT_COLUMN_NAME]\n",
    "    true_sdg = row['sdg_title']\n",
    "\n",
    "    \n",
    "    text_embedding = model.encode(text_to_evaluate, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarities between the text embedding and all SDG title embeddings\n",
    "    similarities = util.cos_sim(text_embedding, sdg_title_embeddings)\n",
    "    \n",
    "    # Find the SDG title with the highest similarity\n",
    "    best_match_idx = similarities.argmax().item()\n",
    "    predicted_sdg = all_sdg_title_list_eval[best_match_idx]\n",
    "    \n",
    "    predictions.append(predicted_sdg)\n",
    "    true_labels.append(str(ii))\n",
    "    ii+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "194bdaf8-5051-4c7c-bbdc-ee1e13e6da0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Classification Metrics ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [16, 6354]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Ensure labels for classification_report are present in both true and predicted\u001b[39;00m\n\u001b[1;32m      4\u001b[0m unique_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(true_labels \u001b[38;5;241m+\u001b[39m predictions)))\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munique_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Confusion Matrix ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# To make the confusion matrix more readable, especially with long SDG titles,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# we can use SDG numbers or shorter names if preferred. Here, we'll use the full titles.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Consider mapping to SDG numbers (1-17) for a more compact matrix if needed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2671\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m \n\u001b[1;32m   2565\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2667\u001b[0m \u001b[38;5;124;03m<BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2668\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2670\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[0;32m-> 2671\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2674\u001b[0m     labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/sklearn/utils/validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    478\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [16, 6354]"
     ]
    }
   ],
   "source": [
    "# --- Calculate Metrics ---\n",
    "print(\"\\n--- Classification Metrics ---\")\n",
    "# Ensure labels for classification_report are present in both true and predicted\n",
    "unique_labels = sorted(list(set(true_labels + predictions)))\n",
    "\n",
    "print(classification_report(true_labels, predictions, labels=unique_labels, zero_division=0))\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "# To make the confusion matrix more readable, especially with long SDG titles,\n",
    "# we can use SDG numbers or shorter names if preferred. Here, we'll use the full titles.\n",
    "# Consider mapping to SDG numbers (1-17) for a more compact matrix if needed.\n",
    "conf_matrix = confusion_matrix(true_labels, predictions, labels=unique_labels)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=unique_labels, columns=unique_labels)\n",
    "print(conf_matrix_df)\n",
    "\n",
    "# For TP, FP, TN, FN, we need to consider it on a per-class (per-SDG) basis.\n",
    "print(\"\\n--- Per-SDG TP, FP, FN, TN ---\")\n",
    "# This requires a multi-label interpretation or one-vs-rest for each SDG.\n",
    "# The confusion matrix already provides TP, FP, FN for each class.\n",
    "# TP(i) = conf_matrix[i, i]\n",
    "# FP(i) = sum(conf_matrix[:, i]) - conf_matrix[i, i]\n",
    "# FN(i) = sum(conf_matrix[i, :]) - conf_matrix[i, i]\n",
    "# TN(i) = sum(all elements) - TP(i) - FP(i) - FN(i)  OR sum of all non-i row and non-i column elements\n",
    "\n",
    "metrics_per_sdg = {}\n",
    "total_samples = len(true_labels)\n",
    "\n",
    "for i, sdg_label in enumerate(unique_labels):\n",
    "    TP = conf_matrix[i, i]\n",
    "    \n",
    "    # Calculate FP: sum of the column i, excluding TP\n",
    "    FP = np.sum(conf_matrix[:, i]) - TP\n",
    "    \n",
    "    # Calculate FN: sum of the row i, excluding TP\n",
    "    FN = np.sum(conf_matrix[i, :]) - TP\n",
    "    \n",
    "    # Calculate TN: sum of all cells not in row i or column i\n",
    "    # TN = total_samples - TP - FP - FN (This is more intuitive for binary cases, for multiclass sum of matrix - row_sum - col_sum + TP)\n",
    "    TN = np.sum(conf_matrix) - (np.sum(conf_matrix[i,:]) + np.sum(conf_matrix[:,i]) - TP)\n",
    "\n",
    "    metrics_per_sdg[sdg_label] = {'TP': TP, 'FP': FP, 'FN': FN, 'TN': TN}\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_per_sdg, orient='index')\n",
    "print(metrics_df)\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"\\nOverall Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8eca6566-6174-47f6-8a35-41098f0bf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels=[]\n",
    "for ii in range(1,17):\n",
    "    true_labels.append(str(ii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e14c9-4f10-4563-a84d-41f834c6c9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
