{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e51266-9052-4b8e-8619-af39a57c98ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.7.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.7.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (175.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.8/175.8 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-2.7.0+cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced591bb-ae0a-4a2b-849a-34b7825cc5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed hf-xet-1.1.0 huggingface-hub-0.31.1 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c8496b-ad76-4460-aa80-42a4193f49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc49d4d-6003-4d0c-995e-7cbacb9a9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------\n",
    "\n",
    "MODEL_NAME = \"facebook/bart-large-mnli\"  # Lightweight zero-shot LLM\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "TOP_K = 3\n",
    "MAX_TOKENS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "781a3ca6-0627-429a-8445-323f9f2aa05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# SDG CANDIDATE LABELS (flattened)\n",
    "# ----------------------------\n",
    "\n",
    "sdg_labels = [\n",
    "    \"No Poverty\",\n",
    "    \"Zero Hunger\",\n",
    "    \"Good Health and Well-being\",\n",
    "    \"Quality Education\",\n",
    "    \"Gender Equality\",\n",
    "    \"Clean Water and Sanitation\",\n",
    "    \"Affordable and Clean Energy\",\n",
    "    \"Decent Work and Economic Growth\",\n",
    "    \"Industry, Innovation and Infrastructure\",\n",
    "    \"Reduced Inequalities\",\n",
    "    \"Sustainable Cities and Communities\",\n",
    "    \"Responsible Consumption and Production\",\n",
    "    \"Climate Action\",\n",
    "    \"Life Below Water\",\n",
    "    \"Life on Land\",\n",
    "    \"Peace, Justice and Strong Institutions\",\n",
    "    \"Partnerships for the Goals\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681a54e8-4f5b-4f61-9991-433f66d3858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# LOAD AND PARSE INPUT FILE\n",
    "# ----------------------------\n",
    "\n",
    "def load_dat_file(path: str) -> List[dict]:\n",
    "    documents = []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                ep, text = line.strip().split('\\t', 1)\n",
    "                documents.append({'ep': ep.strip(), 'text': text.strip()})\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f782d1-c9dc-420f-b653-d39243d8926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# CLASSIFIER FUNCTION\n",
    "# ----------------------------\n",
    "\n",
    "def classify_with_llm(text: str, labels: List[str], classifier, top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "    result = classifier(text, candidate_labels=labels, multi_label=True)\n",
    "    paired = list(zip(result['labels'], result['scores']))\n",
    "    return sorted(paired, key=lambda x: -x[1])[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d288aa44-e32b-4101-9fc3-0b10d2d50c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# MAIN FUNCTION\n",
    "# ----------------------------\n",
    "\n",
    "def run_sdg_classification(input_path, goal_path, output_path):\n",
    "    # Load SDG goals from pickle\n",
    "    with open(goal_path, 'rb') as f:\n",
    "        sdg_df = pickle.load(f)\n",
    "    sdg_labels = sdg_df.iloc[:, 0].tolist()  # assumes SDG names are in first column\n",
    "\n",
    "    # Load patent texts\n",
    "    data = []\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \"\\t\" in line:\n",
    "                ep, text = line.strip().split(\"\\t\", 1)\n",
    "                data.append({\"ep\": ep, \"text\": text})\n",
    "\n",
    "    # Load model\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "    # Classify each text\n",
    "    results = []\n",
    "    for doc in data:\n",
    "        text = \" \".join(doc[\"text\"].split()[:512])  # truncate long text\n",
    "        pred = classifier(text, candidate_labels=sdg_labels, multi_label=True)\n",
    "        results.append({\n",
    "            \"ep\": doc[\"ep\"],\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"predictions\": list(zip(pred[\"labels\"], pred[\"scores\"]))\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\" Classification done. Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2709e169-f6eb-462b-b64b-4fbef9889e55",
   "metadata": {},
   "source": [
    "### EXAMPLE RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49f9a32f-5b77-4a8b-9d04-3d1df651f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "def load_list(filename):\n",
    "    \"\"\"\n",
    "    Loads a gzipped JSON Lines (jsonl) file and returns a list of dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): The filename of the gzipped jsonl file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries read from the file.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            result.append(json.loads(line))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3c4eb6e-387e-4510-9e27-7e3064846d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=load_list(\"test.dat.gz\")\n",
    "with open(\"test1.dat\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in texts:\n",
    "        f.write(text[\"id\"]+\"\\t\"+text[\"text\"]+\"\\n\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c450d292-94cd-4dbe-a0c5-fc9364c02a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification done. Results saved to results.json\n"
     ]
    }
   ],
   "source": [
    "run_sdg_classification(\"test1.dat\", \"sgd_goals.dat\", \"results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f121bba-0a63-4117-a5ce-a5e55f7d6f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Patent: EP3268034A220180117\n",
      " - Reduced Inequalities: 0.16\n",
      " - Partnerships for the Goals: 0.07\n",
      " - Good Health and Well-being: 0.06\n",
      " - Responsible Consumption and Production: 0.03\n",
      " - Life on Land: 0.02\n",
      " - No Poverty: 0.01\n",
      " - Life Below Water: 0.01\n",
      " - Decent Work and Economic Growth: 0.01\n",
      " - Zero Hunger: 0.00\n",
      " - Industry, Innovation and Infrastructure: 0.00\n",
      " - Sustainable Cities and Communities: 0.00\n",
      " - Quality Education: 0.00\n",
      " - Climate Action: 0.00\n",
      " - Gender Equality: 0.00\n",
      " - Peace, Justice and Strong Institutions: 0.00\n",
      " - Affordable and Clean Energy: 0.00\n",
      " - Clean Water and Sanitation: 0.00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "for entry in results:\n",
    "    print(f\"\\nPatent: {entry['ep']}\")\n",
    "    for goal, score in entry['predictions']:\n",
    "        print(f\" - {goal}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b39bf2-af45-4301-bc12-3adde84806b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
